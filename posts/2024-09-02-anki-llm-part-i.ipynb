{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceb5417a-81b7-4406-b389-f67b6c3a5170",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "title: \"Using LLMs to build\"\n",
    "author: \"Gianluca Rossi\"\n",
    "date: \"2024-09-30\"\n",
    "draft: true\n",
    "categories: [genai, llm, anki]\n",
    "format:\n",
    "    html:\n",
    "        code-fold: true\n",
    "jupyter: python3\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e08bf-f22d-41a4-b8c1-3465ad3544d8",
   "metadata": {},
   "source": [
    "## Premise\n",
    "I recently started a new pet project to showcase how easy it is to use small local LLM to build powerful apps, with a particular attention to how to test GenAI applications. This is a topic I see often neglected when chatting with fellow practitioners. My goal with this series of articles is to shade a light into how you can leverage LLMs to build anything you want. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d4a088-6a16-47d0-9bff-aaf57415bc1a",
   "metadata": {},
   "source": [
    "## Motivation\n",
    "In the past month or so, I've been dedicating the little spare time I had between being a dad to a 5-month-old baby and working to building an AI assistant to help curate my Anki deck. If you're unfamiliar with Anki, you can start [here](https://apps.ankiweb.net/). \n",
    "\n",
    "The desktop client and mobile app are great tools for studying you're deck, but they lack critical features for curating your notes, such as:\n",
    "\n",
    "* Edit existing notes to be more concise and improve and standardize formatting\n",
    "* Identify duplicate notes using semantic search\n",
    "* Suggest when a note should be split into multiple atomic notes\n",
    "* Create new notes from a text file or pdf\n",
    "* Identify missing tags\n",
    "* Fact-check notes to ensure there are no factual mistakes\n",
    "* Suggest how existing notes could be improved, e.g., decomposing a note into multiple atomic notes, creating reverse cards, adding a mnemonic, etc.\n",
    "\n",
    "I thought this could have been a great opportunity to show how small local LLMs can be leveraged, and also showcase some best practices that you can take away and implement on your own project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8da157-03dc-4ce5-aaa7-17955047907f",
   "metadata": {},
   "source": [
    "## Improving existing notes with `Llama-3.1-8B-Instruct` and XXX\n",
    "When you start a new project, my advice is to always try to use some very simple prompt engineering, and see how far that can take you. Our first feature is to analyze existing notes and make them concise, simple, and distinct. This will ease learning and help us recalling.\n",
    "\n",
    "We also want notes to comply with some formatting standards, like using Markdown syntax, XXX."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90030a-0087-48f0-9c83-45671401530c",
   "metadata": {},
   "source": [
    "## LLM as a judge\n",
    "\"Vibe checks\" can only take you so far and are certainly not reliable as you start handling more cases. As your application starts to support more features, it can take a lot of time to manually validate your solution handles the new feature, while also not introducing any regression. \n",
    "\n",
    "The more robust \"human eval\" is also not practical as part of the daily development cycle, as it can take a lot of time to assemble a group of reliable human evaluators, instruct them, and ask them to validate the output of your model on a test set. This is certainly something we want to do, but at a much more infrequent basis. \n",
    "\n",
    "A new alternative solution to this problem is to use LLMs as a judge. Many here might raise an eyebrow or two, but I firmly believe it is a great tool in your toolbox if you know its limit, and how to prevent them. The main issue here is that asking an LLM to validate the output of another LLM can be unrealible and, more importantly, not aligned with human preference.\n",
    "\n",
    "To address this issue, I'm proposing to validate on a regular basis that our LLM judge continues to, not just score samples as a human judge would, but also provide the same motivation. Both are critical to ensure the LLM judge can be trusted to generalize to unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912aef57-8e88-45c3-9812-4a9c25d2563b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
