[
  {
    "objectID": "posts/2018-03-21-entropy.html",
    "href": "posts/2018-03-21-entropy.html",
    "title": "A brief introduction about Entropy",
    "section": "",
    "text": "Information Theory is a cornerstone of Machine Learning, yet most Data Science practitioners haven’t been formally trained on it. This blog post is meant to give a brief introduction about entropy, a foundamental measure of uncertainty, which is used in many popular Machine Learning algorithms.\n\nInformation theory studies the quantification, storage, and communication of information. ― cit. Wikipedia\n\nEntropy is a measure of randomness or, in other words, unpredictability. The higher the entropy, the more unpredictable an outcome is. This also represents the minimum expected code length necessary to encode samples of the random variable. Intuitively, the lower the entropy, the easier is to encode a random variable.\nSome popular Machine Learning algorithms ― e.g. Decision Tree, Random Forest, Gradient Boosting, XGBoost ― use entropy to decide when to split a leaf into two leaves.\nLet’s look at how entropy works using a very simple example. Let’s say we are playing a game which involves extracting marbles from an urn. Marbles can be only of two colors, either red or blue. The proportion of red and blue marbles is 70%-30%. How can we measure the uncertainty of this game? Entropy, can come to our rescue.\nEntropy, usually represented with the letter $ H $, is mathematically defined as:\n\\[\n\\begin{align}\n    H(X) &= \\sum_{i=1}^N P(x_i) \\log P\\left(\\frac{1}{x_i}\\right) \\\\\n    &= - \\sum_{i=1}^N P(x_i) \\log P(x_i)\n\\end{align}\n\\]\nIn our example, this would be:\n\\[\n\\begin{align}\n    H(X) &= - \\sum_{i=1}^{2} P(x_i) \\log P(x_i) \\\\\n    &= - P(x_i=blue) \\log P(x_i=blue) - P(x_i=red) \\log P(x_i=red) \\\\\n    &= - .30 \\log 0.30 - .70 \\log 0.70 \\\\\n    &= .610864\n\\end{align}    \n\\]\nIf you find it easier to read code than maths, we could define the entropy in Python with this simple function.\n\n\nCode\nimport numpy as np\n\ndef entropy(p):\n    \"\"\"Measure of randomness in Information Theory.\n\n    Args:\n        p (iterable): The probability of success.\n\n    Returns:\n        (float) The information entropy.\n    \"\"\"\n    return - np.sum([p_i * np.log(p_i) for p_i in p])\n\n\nThen, we could open a Python interpreter and execute the following commands to verify the results.\n\n\nCode\np = [.30, .70]\nentropy(p)\n\n\n0.6108643020548935\n\n\nWe haven’t yet mentioned the unit of measure of entropy, the “bit” (alternatively called “shannon”). The unit of measure of the entropy shouldn’t be confused with the binary digit.\n\nConfusion often arises because the words bit and binary digit are used interchangeably. But, within information theory, a bit and a binary digit are fundamentally different types of entities. A binary digit is a number that can adopt one of two possible values (0 or 1), whereas a bit is the maximum amount of information that can be conveyed by a binary digit. By analogy, a binary digit is like a container, whereas information is the amount of matter in the container. ― cit. Wikipedia\n\nIt should also be noted that, when $ P(x_i) = 0 $ for some $ i $, the value of the corresponding summand $ 0 (0) $ is taken to be $ 0 $.\nIn the next blog post I’ll show some applications of entropy in Machine Learning and Bayesian inference."
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html",
    "href": "posts/2023-04-17-chinchilla.html",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "",
    "text": "In a paper published in 2022, researchers from DeepMind showed how we can extrapolate the ideal model size (N) and pre-training dataset size (D) to achieve thelowest possible validation loss given a predefined compute budget."
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#whats-new",
    "href": "posts/2023-04-17-chinchilla.html#whats-new",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "",
    "text": "In a paper published in 2022, researchers from DeepMind showed how we can extrapolate the ideal model size (N) and pre-training dataset size (D) to achieve thelowest possible validation loss given a predefined compute budget."
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#how-does-it-works",
    "href": "posts/2023-04-17-chinchilla.html#how-does-it-works",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "How does it works?",
    "text": "How does it works?\nThe team at DeepMind did run over 400 experiments with models ranging from 0.5B to 17B parameters and different dataset sizes. Critically, they chose these quantities while keeping fixed computing budgets, in order to later fit three models to estimate the optimal values for N and D to minimize the validation loss. The team used three approaches to estimating the optimal value for those quantities. All three approaches converged to very similar solutions, where doubling the size of the model should require doubling also the size of the training dataset.\n\nTo prove the validity of their findings, they trained a new LLM, named Chinchilla (70B parameters, 1.4T tokens), which was able to outperform much larger models like GPT-3 (175B parameters, 300B tokens) and Gopher (280B parameters, 300B tokens) in a series of benchmarks."
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#why-it-matters",
    "href": "posts/2023-04-17-chinchilla.html#why-it-matters",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "Why it matters?",
    "text": "Why it matters?\nThis work shows how LLMs trained in the past few years are not compute optimal, and could have achieved better results by selecting a smaller model size and much large pre-training dataset size. This is a significant results because it shows that much smaller models, that are cheaper to train and use at inference time, can be competitive and even outperform much larger models trained on less data."
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#what-do-we-think",
    "href": "posts/2023-04-17-chinchilla.html#what-do-we-think",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "What do we think?",
    "text": "What do we think?\nCompute optimal models are not necessarily the best models. As shown in recent work from XXX, given a fixed model size, training a model with even more tokens than what suggested in this paper, can lead to an even better model, while suffering minimal computing overhead. That said, the scaling law published by DeepMind can give us a good starting point to select a smaller model and dataset size to iterate fast, before confidently scale the model and dataset size"
  },
  {
    "objectID": "posts/2018-02-26-welcome.html",
    "href": "posts/2018-02-26-welcome.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "Welcome to my blog!\nMy name is Gianluca — although my friends call me Luca!\nI’ve been passionate about Machine Learning since 2013, when I first came across it at my first job after college in a digital advertising company. After starting as a Campaign Analyst, I was lucky enough to join the Machine Learning team and got hooked in. Since then, I’ve been on a journey to learn more about AI and ML.\n\nThis is blog is meant to be a space for me to explain some interesting ML concepts and share some of my own research. I believe the best way of learning something is to teach it to someone else, and a blog is a good way to accomplish that.\nI hope you enjoy my writing and share your comments and feedback.\nBest,\nLuca"
  },
  {
    "objectID": "posts/2018-10-28-1cycle.html",
    "href": "posts/2018-10-28-1cycle.html",
    "title": "The 1cycle policy",
    "section": "",
    "text": "In his most recent work, Leslie N. Smith gives extremely useful practical advice on how to train neural networks and tune the most important hyper-parameters. In his experiments, Smith was able to improve the current State Of The Art (SOTA) results on a number of datasets and network combinations, including the popular CIFAR-10, CIFAR-100, MNIST, and ImageNet.\nSmith argues that Learning Rate (LR), Momentum, batch size, and weight decay should be tuned synchronously to find the best configuration that minizines the validation loss and maximize validation accuracy. Such intuitive remark has however been elusive for many researchers in the recent past due to the computational cost associated with such strategies. An extensive grid search across a 4-dimensional hyper-parameters space would, in fact, be extremely expensive ― especially for the average user who doesn’t have at his disposal a cluster of GPUs for running experiments.\nLeslie Smith goal is therefore to both shade a light on the intrinsic dependencies between such hyper-parameters and presents a good heuristic to rapidly identify the best combination. In addition to that, Smith also gives easy to follow rules of thumbs regarding good starting hyper-parameters to use when approaching a new dataset or architecture, and hints at how to improve on the initial configuration. In this paper, Smith also introduces what he calls the 1cycle policy, a strategy to train neural networks which shows super-convergence capabilities in many datasets if hyper-parameters are correctly chosen.\n\nUnderfitting and Overfitting\nSmith points out how, to minimize the prediction error, we need to identify the sweet spot between underfitting and overfitting.\n\n\n\nA chart showing the relationship between underfitting and overfitting\n\n\nHe argues that hyper-parameter configurations which more rapidly reduce the validation loss in the first few epochs and then plateau are generally best, since they:\n\nEmpirically reach lower validation loss\nDramatically reduce computing time ― that can, therefore, be allocated to experiment other things\n\nSmith stresses how large Learning Rate values are desirable since they reduce underfitting and help to regularize the training. On other hand, Smith reminds the reader that extremely high values of Learning Rate can cause the training to diverge. To quickly identify reasonable minimum and maximum boundary values for the Learning Rates with only a few epochs, Smith suggests to use the “LR range test”, which he proposed in an earlier paper.\n\n\nThe 1cycle policy\nSuch a policy requires the coordinated change of Learning Rate and Momentum during training time. During the course of over 1,000 experiment, Smith noticed how a strategy of one cycle length wherein the first half we increase the LR while decrease Momentum, and then revert the trends in the second half of the cycle, seems to perform particularly well.\n\n\n\nA chart showing the learning rate and momentum schedule in the 1cycle policy\n\n\nIn Smith experiments, such policy outperforms in terms of validation score any other previously known training policy, and achieves optimal results in a fraction of the necessary epochs.\n\n\nBatch size, weight decay, and dropout\nSmall batch sizes, weight decay, and dropout are other popular techniques to regularize neural networks. Smith argues that these regularization techniques should be reduced in favor of using larger learning rates. Larger learning rates, in fact, provide as good regularization properties and, additionally, allow convergence in a shorter number of iterations.\nSmith suggested to use the largest batch size allowed by our server and to try a few different combinations of weight decay and dropout while comparing the results using the “LR range test”.\n\n\nGeneralization to different network architectures and dataset\nFinally, Smith explores how his recommendations generalize to different network architectures and datasets. Interestingly, he shows how his recommendations hold not just for shallow and residual networks but also in deep architectures like densenets, and hyper-densenets. He also shares the code he used to improve the results presented in numerous studies while also considerably reducing training time. This is extremely important to further validate the hypothesis that the 1cycle policy is the best currently known general strategy to optimize both shallow and deep networks architectures.\n\n\nConclusions\nThe recent work of Leslie N. Smith has been fundaments to democratize deep learning and achieving SOTA results on single and multi-GPU configurations. The “LR range test”, 1cycle policy, and the heuristics to tune Learning Rate, Momentum, batch size and weight decay presented in this paper are advancing what previously presented in the literature."
  },
  {
    "objectID": "posts/2018-09-14-statistical-rethinking.html",
    "href": "posts/2018-09-14-statistical-rethinking.html",
    "title": "Statistical Rethinking",
    "section": "",
    "text": "statistical-rethinking-cover\n\n\nAfter reading the excellent reviews in Amazon, I decided to give it a go to the text-book Statistical Rethinking by Richard McElreath. Long story short, I can’t praise enough the author for this great book that I wish I have discovered long time ago, when I first started doing Bayesian data analysis.\nThe target reader for the book is a data analyst or data scientist with only a basic understanding on Bayesian inference. The author does a wonderful job of making sure to guide the reader from very simple concepts up to some intermediate level modeling techniques, like Multilevel Modeling. I’ve particularly appreciated the presence of plenty of excercises and the optional paragraphs where more advanced concepts are described ― without getting in the way of less experienced readers that want to get as much as possible from their first read of the material.\nThe book also contains lots of examples, using R and a companion open-source library named after the book. Even though I prefer much more the Python scientific eco-system for doing Data Science, and I haven’t touched R for quite a few years now, following the examples is very easy. The author also encapsuled all of the Stan models into it’s own wrapper library. I personally would have appreciated to see more Stan code, which I find quite easily interpretable, and would be anyway the language of choice for many readers after completing this book. Nonetheless, the companion library seems to be very compelling and will centainly bring you through all the examples in the book without much effort. For more advanced models though, Stan offers more flexibility.\nRegarding the content, I particularly appreciated the chapters about Information Theory and Multilevel models. These are probably the most advanced ones, but they are easy to follow and quite engaging also for more experienced users.\nThroughout the entire book, McElreath reiterate the importance of using counterfactual plots to understand the absolute effect of a feature on the target variable, and WAIC to compare alternative models. This is something I’ve particularly appreciated and I believe it should be promoted more often. I wish these kind of advices were given to me when I started my journey into Bayesian inference.\nTo summarize, Statistical Rethinking is a very enjoyable book, packed with important lessons for both beginner and intermediate practitioners who want to better understand how to do proper Bayesian data analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Yet Another Blog about Machine Learning",
    "section": "",
    "text": "Paper review: Training Compute-Optimal Large Language Models\n\n\n\n\n\n\n\narxiv\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nGianluca Rossi\n\n\n\n\n\n\n  \n\n\n\n\nThe 1cycle policy\n\n\n\n\n\n\n\narxiv\n\n\n\n\n\n\n\n\n\n\n\nOct 28, 2018\n\n\nGianluca Rossi\n\n\n\n\n\n\n  \n\n\n\n\nStatistical Rethinking\n\n\n\n\n\n\n\nbooks\n\n\nstatistics\n\n\nbayes\n\n\n\n\n\n\n\n\n\n\n\nSep 14, 2018\n\n\nGianluca Rossi\n\n\n\n\n\n\n  \n\n\n\n\nA brief introduction about Entropy\n\n\n\n\n\n\n\ninformation-theory\n\n\n\n\n\n\n\n\n\n\n\nMar 21, 2018\n\n\nGianluca Rossi\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to my blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nFeb 26, 2018\n\n\nGianluca Rossi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Yet another blog about AI/ML :-)\nWelcome to my blog!\nMy name is Gianluca — although my friends call me Luca!\nI’ve been passionate about Machine Learning since 2013, when I first came across it at my first job after college in a digital advertising company. After starting as a Campaign Analyst, I was lucky enough to join the Machine Learning team and got hooked in. Since then, I’ve been on a journey to learn more about AI and ML.\nAlong this journey I’ve been lucky enough to meet some fantastic people and learning a lot from them.\nThis is blog is meant to be a space for me to explain some interesting ML concepts and share some of my own research. I believe the best way of learning something is to teach it to someone else, and a blog is a good way to accomplish that.\nI hope you enjoy my writing and share your comments and feedback.\nBest,\nLuca"
  }
]