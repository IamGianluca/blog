[
  {
    "objectID": "posts/2023-04-17-chinchilla.html",
    "href": "posts/2023-04-17-chinchilla.html",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "",
    "text": "Link: Training Compute-Optimal Large Language Models"
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#whats-new",
    "href": "posts/2023-04-17-chinchilla.html#whats-new",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "What’s new?",
    "text": "What’s new?\nIn a paper published in 2022, researchers from DeepMind showed how we can extrapolate the ideal model size (N) and pre-training dataset size (D) to achieve thelowest possible validation loss given a predefined compute budget."
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#how-does-it-works",
    "href": "posts/2023-04-17-chinchilla.html#how-does-it-works",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "How does it works?",
    "text": "How does it works?\nThe team at DeepMind did run over 400 experiments with models ranging from 0.5B to 17B parameters and different dataset sizes. Critically, they chose these quantities while keeping fixed computing budgets, in order to later fit three models to estimate the optimal values for N and D to minimize the validation loss. The team used three approaches to estimating the optimal value for those quantities. All three approaches converged to very similar solutions, where doubling the size of the model should require doubling also the size of the training dataset.\n\n\n\nimage.png\n\n\nTo prove the validity of their findings, they trained a new LLM, named Chinchilla (70B parameters, 1.4T tokens), which was able to outperform much larger models like GPT-3 (175B parameters, 300B tokens) and Gopher (280B parameters, 300B tokens) in a series of benchmarks.\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#why-it-matters",
    "href": "posts/2023-04-17-chinchilla.html#why-it-matters",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "Why it matters?",
    "text": "Why it matters?\nThis work shows how LLMs trained in the past few years are not compute optimal, and could have achieved better results by selecting a smaller model size and much large pre-training dataset size. This is a significant results because it shows that much smaller models, that are cheaper to train and use at inference time, can be competitive and even outperform much larger models trained on less data."
  },
  {
    "objectID": "posts/2023-04-17-chinchilla.html#what-do-we-think",
    "href": "posts/2023-04-17-chinchilla.html#what-do-we-think",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "What do we think?",
    "text": "What do we think?\nCompute optimal models are not necessarily the best models. As shown in recent work from XXX, given a fixed model size, training a model with even more tokens than what suggested in this paper, can lead to an even better model, while suffering minimal computing overhead. That said, the scaling law published by DeepMind can give us a good starting point to select a smaller model and dataset size to iterate fast, before confidently scale the model and dataset size"
  },
  {
    "objectID": "posts/2023-04-16-welcome.html",
    "href": "posts/2023-04-16-welcome.html",
    "title": "Welcome to my blog",
    "section": "",
    "text": "Welcome to my blog!\nMy name is Gianluca — although my friends call me Luca!\nI’ve been passionate about Machine Learning since 2013, when I first came across it at my first job after college in a digital advertising company. After starting as a Campaign Analyst, I was lucky enough to join the Machine Learning team and got hooked in. Since then, I’ve been on a journey to learn more about AI and ML.\n\n\n\ngithub-profile-pic.jpeg\n\n\nThis is blog is meant to be a space for me to explain some interesting ML concepts and share some of my own research. I believe the best way of learning something is to teach it to someone else, and a blog is a good way to accomplish that.\nI hope you enjoy my writing and share your comments and feedback.\nBest,\nLuca"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Paper review: Training Compute-Optimal Large Language Models\n\n\n\n\n\n\n\n\n\n\n\n\nApr 17, 2023\n\n\nGianluca Rossi\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to my blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 16, 2023\n\n\nGianluca Rossi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Yet another blog about AI/ML :-)"
  }
]