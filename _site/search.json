[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2023-04-16-chinchilla.html",
    "href": "posts/2023-04-16-chinchilla.html",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "",
    "text": "Link: Training Compute-Optimal Large Language Models"
  },
  {
    "objectID": "posts/2023-04-16-chinchilla.html#whats-new",
    "href": "posts/2023-04-16-chinchilla.html#whats-new",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "What’s new?",
    "text": "What’s new?\nIn a paper published in 2022, researchers from DeepMind showed how we can extrapolate the ideal model size (N) and pre-training dataset size (D) to achieve thelowest possible validation loss given a predefined compute budget."
  },
  {
    "objectID": "posts/2023-04-16-chinchilla.html#how-does-it-works",
    "href": "posts/2023-04-16-chinchilla.html#how-does-it-works",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "How does it works?",
    "text": "How does it works?\nThe team at DeepMind did run over 400 experiments with models ranging from 0.5B to 17B parameters and different dataset sizes. Critically, they chose these quantities while keeping fixed computing budgets, in order to later fit three models to estimate the optimal values for N and D to minimize the validation loss. The team used three approaches to estimating the optimal value for those quantities. All three approaches converged to very similar solutions, where doubling the size of the model should require doubling also the size of the training dataset.\n\n\n\nimage.png\n\n\nTo prove the validity of their findings, they trained a new LLM, named Chinchilla (70B parameters, 1.4T tokens), which was able to outperform much larger models like GPT-3 (175B parameters, 300B tokens) and Gopher (280B parameters, 300B tokens) in a series of benchmarks.\n\n\n\nimage.png"
  },
  {
    "objectID": "posts/2023-04-16-chinchilla.html#why-it-matters",
    "href": "posts/2023-04-16-chinchilla.html#why-it-matters",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "Why it matters?",
    "text": "Why it matters?\nThis work shows how LLMs trained in the past few years are not compute optimal, and could have achieved better results by selecting a smaller model size and much large pre-training dataset size. This is a significant results because it shows that much smaller models, that are cheaper to train and use at inference time, can be competitive and even outperform much larger models trained on less data."
  },
  {
    "objectID": "posts/2023-04-16-chinchilla.html#what-do-we-think",
    "href": "posts/2023-04-16-chinchilla.html#what-do-we-think",
    "title": "Paper review: Training Compute-Optimal Large Language Models",
    "section": "What do we think?",
    "text": "What do we think?\nCompute optimal models are not necessarily the best models. As shown in recent work from XXX, given a fixed model size, training a model with even more tokens than what suggested in this paper, can lead to an even better model, while suffering minimal computing overhead. That said, the scaling law published by DeepMind can give us a good starting point to select a smaller model and dataset size to iterate fast, before confidently scale the model and dataset size"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "blog",
    "section": "",
    "text": "Post With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 14, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 11, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nPaper review: Training Compute-Optimal Large Language Models\n\n\n\n\n\n\n\n\n\n\n\n\nInvalid Date\n\n\nGianluca Rossi\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Yet another blog about AI/ML :-)"
  }
]